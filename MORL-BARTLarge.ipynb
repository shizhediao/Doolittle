{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORL Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize all metrics that will be used in the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('metrics/SemanticSimilarity/')\n",
    "sys.path.append('metrics/BARTScore/')\n",
    "\n",
    "sent = ['When applying the Naturalistic Driving Film in the design process, there are several aspects that need to be taken into consideration.']\n",
    "ref = ['When apply Naturalistic Driving Film into the design process, there are several aspects need to take into consideration.',]\n",
    "\n",
    "import torch\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Semantic Similarity (SIM)\n",
    "Download needed model files: sim.pt and sim.sp.30k.model from https://github.com/martiansideofthemoon/style-transfer-paraphrase/tree/master/style_paraphrase/evaluation. \n",
    "And modify the corresponding pathes in test_sim.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_sim import find_similarity\n",
    "\n",
    "#test find_similarity\n",
    "sim_score = find_similarity(sent,ref)\n",
    "\n",
    "print(sim_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Perplexity (PPL)\n",
    "Tune your own GPT2-Large model and put the tuned model under [metrics/Perplexity/Model-for-PPL](metrics/Perplexity/Model-for-PPL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "perplexity = evaluate.load(\"perplexity\", module_type=\"metric\",cache_dir = None)\n",
    "LM_path = \"metrics/Perplexity/Model-for-PPL\"\n",
    "\n",
    "input_texts = sent\n",
    "\n",
    "#test PPL calculation\n",
    "results = perplexity.compute(model_id=LM_path,\n",
    "                             add_start_token=False,\n",
    "                             predictions=input_texts)\n",
    "\n",
    "print(list(results.keys()))\n",
    "print(round(results[\"mean_perplexity\"], 2))\n",
    "print(round(results[\"perplexities\"][0], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize BartScorer (BARTS)\n",
    "Download needed model file: bart_score.pth from https://github.com/neulab/BARTScore and put it under [metrics/BARTScore/Model-for-BARTS/](.metrics/BARTScore/Model-for-BARTS/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bart_score import BARTScorer\n",
    "\n",
    "#test BARTScorer\n",
    "bart_scorer = BARTScorer(device = \"cuda:0\" if device == 0 else \"cpu\", checkpoint='facebook/bart-large-cnn')\n",
    "bart_scorer.load(path='metrics/BARTScore/Model-for-BARTS/bart_score.pth')\n",
    "bart_score = bart_scorer.score(sent,ref)\n",
    "\n",
    "print(bart_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Transfer Accuracy Classifier (ACC)\n",
    "Train your own RoBERTa-Large classifier fine-tuned on the CoLA or AESW2016 dataset following huggingface tutorial and put it under metrics/TransferAccuracy/Model-for-ACC/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "sentiment_pipe = pipeline(\"text-classification\", model=\"metrics/TransferAccuracy/Model-for-ACC\", device= device, tokenizer = AutoTokenizer.from_pretrained('roberta-large', max_length = 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import PPOTrainer, PPOConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PPOConfig(\n",
    "    model_name=\"path to your pretrained bart model\", #path to your pretrained bart-large model for seq2seq text generation\n",
    "    learning_rate=1e-6,\n",
    "    #log_with=\"wandb\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "sent_kwargs = {\n",
    "    \"return_all_scores\": True,\n",
    "    \"function_to_apply\": \"none\",\n",
    "    \"batch_size\": 16\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load AWF data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_awf_dataset(config, dataset_path):\n",
    "    \"\"\"\n",
    "    Build dataset for training. This builds the dataset from `load_dataset`, one should \n",
    "    customize this function to train the model on its own dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name (`str`): \n",
    "            The name of the dataset to be loaded.\n",
    "    \n",
    "    Returns:\n",
    "        dataloader (`torch.utils.data.DataLoader`):\n",
    "            The dataloader for the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "    ds = load_dataset('csv',data_files=dataset_path)['train']\n",
    "    ds = ds.rename_columns({'text': 'review'})\n",
    "\n",
    "    def tokenize(sample):\n",
    "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"][:-1])\n",
    "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"],skip_special_tokens=True)\n",
    "        return sample\n",
    "\n",
    "    ds = ds.map(tokenize, batched=False)\n",
    "    ds.set_format(type='torch')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = build_awf_dataset(config,'AWF-dataset/dev.0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your pre-trained BART-Large language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForSeq2SeqLMWithValueHead\n",
    "\n",
    "model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "ref_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize PPOTrainer\n",
    "The `PPOTrainer` takes care of device placement and optimization later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ppo_trainer.accelerator.device\n",
    "if ppo_trainer.accelerator.num_processes == 1:\n",
    "   device = 0 if torch.cuda.is_available() else \"cpu\" # to avoid a `pipeline` bug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_kwargs = {\n",
    "    \"min_length\":-1,\n",
    "    \"max_length\":256,\n",
    "    \"top_k\": 0.0,\n",
    "    #\"top_p\": 1.0,\n",
    "    'do_sample': True,\n",
    "    'early_stopping' : False, \n",
    "    'num_beams' : 4, \n",
    "    #'no_repeat_ngram_size': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training loop consists of the following main steps:\n",
    "1. Get the generated refined paragraph from the policy network (BART-Large)\n",
    "2. Get the scores of generated paragraph on each chosen metrics\n",
    "3. Get the final reward as a weighted sum of all scores got in Step 2\n",
    "4. Optimize policy with PPO using the (query, response, reward) triplet. Here, query is the input paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "import numpy as np\n",
    "\n",
    "weights = [0.01,1.0,0.01,0.01] #weights for bart-score, -PPL/200, similarity, classification_pipeline\n",
    "\n",
    "for ep in range(5):\n",
    "    for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "        query_tensors = batch['input_ids']\n",
    "\n",
    "        response_tensors = []\n",
    "        for query in query_tensors:\n",
    "            response = ppo_trainer.generate(query, **gen_kwargs)\n",
    "            response_tensors.append(response.squeeze())\n",
    "        batch['response'] = [tokenizer.decode(r.squeeze(),skip_special_tokens=True) for r in response_tensors]\n",
    "\n",
    "        texts = batch['response']\n",
    "        references = batch['review']\n",
    "        \n",
    "        #get BART Score\n",
    "        BS_rewards = [torch.tensor(bart_scorer.score([reference],[output])[0]) for output, reference in zip(texts,references)]\n",
    "        \n",
    "        #get Perplexity Score\n",
    "        PPL_rewards = []\n",
    "        for output in texts:\n",
    "            try:\n",
    "                PPL_rewards.append(torch.tensor(-perplexity.compute(model_id=LM_path,\n",
    "                                                   add_start_token=True, \n",
    "                                                   predictions=[output])[\"mean_perplexity\"]/200.0))\n",
    "            except:\n",
    "                print(\"\"\"Warning: Model generated a paragraph contains less than 2 tokens. If this warning shows frequently, \n",
    "                      the leaning process is probably crushed. Please try to restart this notebook and tune the model again.\"\"\")\n",
    "                PPL_rewards.append(torch.tensor([-10000]))\n",
    "        \n",
    "        #get Semantic Similarity Score\n",
    "        SIM_rewards = [torch.tensor(find_similarity([output],[reference])[0]) for output, reference in zip(texts,references)]\n",
    "        \n",
    "        pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
    "        AF_rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "        #sometimes the below implementation of AF_rewards is better\n",
    "        #AF_rewards = [torch.tensor(exp(output[1][\"score\"])/(exp(output[1][\"score\"])+exp(output[0][\"score\"]))) for output in pipe_outputs]\n",
    "\n",
    "        rewards = [weights[0]*a + weights[1]*b + weights[2]*c + weights[3]*d for a,b,c,d in zip(BS_rewards,PPL_rewards,SIM_rewards,AF_rewards)]\n",
    "        \n",
    "        #### Run PPO step \n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "    print('epoch : ', ep+1)\n",
    "    \n",
    "    #get the generation result on test set after each epoch, comment all lines below to accelerate training (not recommand, the last epoch is unlikely the best)\n",
    "    \n",
    "    with open('AWF-dataset/paragraph_native_test.0') as inp, open('tmp/model_output.txt','w') as out:\n",
    "        lines = inp.readlines()\n",
    "        for sent in lines:\n",
    "            if sent.endswith('/n'):\n",
    "                sent = sent[:-1]\n",
    "            sent_encoded = tokenizer.encode(sent,return_tensors='pt').cuda()\n",
    "            response = model.generate(sent_encoded, **gen_kwargs)\n",
    "            gen_sent = tokenizer.decode(response.squeeze(),skip_special_tokens=True)\n",
    "            out.write(gen_sent+'\\n')\n",
    "\n",
    "    lines = []\n",
    "    \n",
    "    with open('tmp/model_output.txt') as inp, open('tmp/model_output_processed.txt','w',encoding='utf-8') as out:\n",
    "        lines = inp.readlines()\n",
    "        #print(len(lines))\n",
    "        count = 0\n",
    "        for line in lines:\n",
    "            if line.endswith('\\n'):\n",
    "                line = line[:-1]\n",
    "            if line.rfind('.') != -1:\n",
    "                out.write(line[:line.rfind('.')+1]+'\\n')\n",
    "            else:\n",
    "                out.write(line+'\\n')\n",
    "\n",
    "    with open('tmp/model_output.txt') as inp1, open('tmp/model_output_processed.txt') as inp2:\n",
    "        with open('tmp/model_output-'+str(ep+1)+'.txt','w',encoding='utf-8') as out1, open('tmp/model_output_processed-'+str(ep+1)+'.txt','w',encoding='utf-8') as out2:\n",
    "            out1.write(inp1.read())\n",
    "            out2.write(inp2.read())\n",
    "    \n",
    "    #evaluate and save model after each epoch, comment all lines below to accelerate training (not recommand, the last epoch is unlikely the best)\n",
    "\n",
    "    #AF score\n",
    "    print('AF score:')\n",
    "    results = []\n",
    "    with open('tmp/model_output.txt') as inp:\n",
    "        preds = sentiment_pipe (inp.readlines())\n",
    "        for pred in preds:\n",
    "            if pred['label'] == 'LABEL_0':\n",
    "                results.append(0)\n",
    "            else:\n",
    "                results.append(1)\n",
    "\n",
    "    AF_1 = round(np.mean(results)*100,2)\n",
    "\n",
    "    print('ACC of raw-output: ' + str(AF_1) + '%.')\n",
    "\n",
    "    results = []\n",
    "    with open('tmp/model_output_processed.txt') as inp:\n",
    "        preds = sentiment_pipe (inp.readlines())\n",
    "        for pred in preds:\n",
    "            if pred['label'] == 'LABEL_0':\n",
    "                results.append(0)\n",
    "            else:\n",
    "                results.append(1)\n",
    "\n",
    "    AF_2 = round(np.mean(results)*100,2)\n",
    "\n",
    "    print('ACC of processed-output: ' + str(AF_2) + '%.')\n",
    "\n",
    "    print()\n",
    "    \n",
    "    #GPT-2 PPL\n",
    "    print('GPT-2 PPL')\n",
    "    with open('tmp/model_output.txt') as inp:\n",
    "        input_texts  = inp.readlines()\n",
    "        results = perplexity.compute(model_id=LM_path,\n",
    "                             add_start_token=True,\n",
    "                             predictions=input_texts)\n",
    "        \n",
    "    PPL_1 = round(results[\"mean_perplexity\"], 2)\n",
    "\n",
    "    print('PPL of raw-output: ', PPL_1)\n",
    "\n",
    "    with open('tmp/model_output_processed.txt') as inp:\n",
    "        input_texts  = inp.readlines()\n",
    "        results = perplexity.compute(model_id=LM_path,\n",
    "                             add_start_token=True,\n",
    "                             predictions=input_texts)\n",
    "        \n",
    "    PPL_2 = round(results[\"mean_perplexity\"], 2)\n",
    "\n",
    "    print('PPL of processed-output: ', PPL_2)\n",
    "\n",
    "    print()\n",
    "\n",
    "    #similarity\n",
    "    print(\"Similarity score:\")\n",
    "    print(\"vs input:\")\n",
    "    SIM_INP_1 = 0\n",
    "    SIM_INP_2 = 0\n",
    "    SIM_REF_1 = 0\n",
    "    SIM_REF_2 = 0\n",
    "    with open('AWF-dataset/paragraph_native_test.0') as inp1, open('tmp/model_output.txt') as inp2:\n",
    "        gen = inp2.readlines()\n",
    "        ref = inp1.readlines()\n",
    "        sim_score = find_similarity(gen,ref)\n",
    "        SIM_INP_1 = round(np.mean(sim_score)*100,2)\n",
    "        print('SIM against input of raw-output: ', SIM_INP_1)\n",
    "    \n",
    "    with open('AWF-dataset/paragraph_native_test.0') as inp1, open('tmp/model_output_processed.txt') as inp2:\n",
    "        gen = inp2.readlines()\n",
    "        ref = inp1.readlines()\n",
    "        sim_score = find_similarity(gen,ref)\n",
    "        SIM_INP_2 = round(np.mean(sim_score)*100,2)\n",
    "        print('SIM against input of processed-output: ', SIM_INP_2)\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(\"vs gold:\")\n",
    "    with open('AWF-dataset/paragraph_native_test.1') as inp1, open('tmp/model_output.txt') as inp2:\n",
    "        gen = inp2.readlines()\n",
    "        ref = inp1.readlines()\n",
    "        sim_score = find_similarity(gen,ref)\n",
    "        SIM_REF_1 = round(np.mean(sim_score)*100,2)\n",
    "        print('SIM against reference of raw-output: ', SIM_REF_1)\n",
    "    \n",
    "    with open('AWF-dataset/paragraph_native_test.1') as inp1, open('tmp/model_output_processed.txt') as inp2:\n",
    "        gen = inp2.readlines()\n",
    "        ref = inp1.readlines()\n",
    "        sim_score = find_similarity(gen,ref)\n",
    "        SIM_REF_2 = round(np.mean(sim_score)*100,2)\n",
    "        print('SIM against reference of Processed-output: ', SIM_REF_2)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    #bart-score\n",
    "    print('BART Score:')\n",
    "    BART_1 = 0\n",
    "    BART_2 = 0\n",
    "    with open('AWF-dataset/paragraph_native_test.0') as inp1, open('tmp/model_output.txt') as inp2:\n",
    "        gen = inp2.readlines()\n",
    "        ref = inp1.readlines()\n",
    "        bart_score = bart_scorer.score(gen,ref)\n",
    "        BART_1 = round(np.mean(bart_score),2)\n",
    "        print('BART Score of raw-output: ', BART_1)\n",
    "    \n",
    "\n",
    "    with open('AWF-dataset/paragraph_native_test.1') as inp1, open('tmp/model_output_processed.txt') as inp2:\n",
    "        gen = inp2.readlines()\n",
    "        ref = inp1.readlines()\n",
    "        bart_score = bart_scorer.score(gen,ref)\n",
    "        BART_2 = round(np.mean(bart_score),2)\n",
    "        print('BART Score of processed-output: ', BART_2)\n",
    "    \n",
    "    print(\"Saving tmp model\")\n",
    "    model.save_pretrained('tmp/tmp-models/epoch-'+str(ep+1), push_to_hub=False)\n",
    "    tokenizer.save_pretrained('tmp/tmp-models/epoch-'+str(ep+1), push_to_hub=False)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving model\")\n",
    "model.save_pretrained('tmp/tmp-models', push_to_hub=False)\n",
    "tokenizer.save_pretrained('tmp/tmp-models', push_to_hub=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "78d9b7fcbdda387ac9d2474c8975e0269b7969b7f8a160d954c0e8e943000346"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
